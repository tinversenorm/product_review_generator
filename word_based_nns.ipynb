{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_based_nns.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "74-miaLGCBSv",
        "colab_type": "code",
        "outputId": "9ee3a216-5abb-4eb9-9afd-b948979f88f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.62)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.62 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.62)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UikQ_H7NYFSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "evvGwRZRYkX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_words(filename='WinePreprocessed.out'):\n",
        "  import numpy as np\n",
        "  return np.loadtxt(filename, dtype=np.str_, delimiter=' ', encoding='utf8')\n",
        "\n",
        "def load_sentences(words, window=10, step=5):\n",
        "  import numpy as np\n",
        "  overlapped = [list(words[x: x + window])\n",
        "                for x in range(0, len(words), 5) if x + window <= len(words)]\n",
        "  x_train = [s[:-1] for s in overlapped]\n",
        "  y_train = [[s[-1]] for s in overlapped]\n",
        "  return x_train, y_train, overlapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T1VUQ-G95lzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wine_words = load_words()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ym7X0pWeay7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wine_x, wine_y, wine_sentences = load_sentences(wine_words, window=20, step=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9RBjmqRNAKqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def embed_words(words, min_count=1, debug=True, sg=0, model='word2vec'):\n",
        "    import gensim\n",
        "    if debug:\n",
        "        print(\"Creating embeddings...\")\n",
        "    if model == 'word2vec':\n",
        "        embed_model = gensim.models.Word2Vec(\n",
        "            words,\n",
        "            size=100, # vector dimension\n",
        "            min_count=min_count, # min num times it needs to be in sentences to count\n",
        "            window=5, # num words around word that affect vector\n",
        "            workers=4,\n",
        "            sg=sg)\n",
        "    else:\n",
        "        embed_model = gensim.models.FastText(\n",
        "            words,\n",
        "            size=100,\n",
        "            min_count=min_count, #tried: min_count 1, min_count 5 difficult = future\n",
        "            window=5, \n",
        "            workers=4,\n",
        "            sg=sg\n",
        "        )\n",
        "    if debug:\n",
        "        print(\"Embedding model created.\")\n",
        "    return embed_model, words \n",
        "\n",
        "def get_embedding_layer(embed_model):\n",
        "    from tensorflow.python.layers.embeddings import Embedding\n",
        "    weights = embed_model.wv.vectors\n",
        "    vocab_size, embedding_size = weights.shape\n",
        "    return Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[weights])\n",
        "\n",
        "def word2index(embed_model, word):\n",
        "    return embed_model.wv.vocab[word].index\n",
        "\n",
        "def index2word(embed_model, index):\n",
        "    return embed_model.wv.index2word[index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q207xuf2Au0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_embeddings(embed_model, x, y):\n",
        "  import numpy as np\n",
        "  return [np.array([word2index(embed_model, w) for w in words]) for words in x], \\\n",
        "          np.array([word2index(embed_model, w[0]) for w in y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_0oYWaEIkSk",
        "colab_type": "code",
        "outputId": "93232eac-45a7-4f30-e381-49825145936e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "embed_model_w2v, wine_sentences = embed_words(wine_sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating embeddings...\n",
            "Embedding model created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oJrKxxHKSnLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed83548b-ceeb-403d-c9b6-bf81955bbfee"
      },
      "cell_type": "code",
      "source": [
        "embed_model_ft_sg, wine_sentences = embed_words(wine_sentences, sg=1, model='fasttext')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "elE02MSgGrDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wine_x, wine_y = convert_to_embeddings(embed_model_w2v, wine_x, wine_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3NKI4zNXGyI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#The Models"
      ]
    },
    {
      "metadata": {
        "id": "Gca6AwfVXJ6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(model, x_train, y_train):\n",
        "    model.fit(x_train, y_train, \n",
        "             batch_size=1024,\n",
        "             epochs=5,\n",
        "             verbose=1)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-UiL_KiOnyuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(model, embed_model, num_words=30, seed_word='this'):\n",
        "    import numpy as np\n",
        "    test = [word2index(embed_model, seed_word)]\n",
        "    vocab_size = embed_model.wv.vectors.shape[0]\n",
        "    while len(test) < num_words:\n",
        "        #print(test)\n",
        "        #print(model.predict_proba(test))\n",
        "        predictions = model.predict_proba(test)[-1]\n",
        "        #next_word = np.argmax(np.linspace(0, vocab_size - 1, vocab_size))\n",
        "        next_word = np.random.choice(np.linspace(0, vocab_size - 1, vocab_size),\n",
        "                                    p=predictions)\n",
        "        test.append(next_word)\n",
        "    return \" \".join([index2word(embed_model, int(w)) for w in test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAxCdNc3XO48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_lstm_model(embed_model, dropout=0.2):\n",
        "    import tensorflow\n",
        "    from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "    from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "    from tensorflow.python.keras.layers import Dense, Activation\n",
        "    from tensorflow.python.keras.models import Sequential\n",
        "    \n",
        "    #things tried: 1 lstm, 2 lstms, 2 lstms with dropout\n",
        "    weights = embed_model.wv.vectors\n",
        "    vocab_size, embedding_size = weights.shape\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[weights]))\n",
        "    model.add(LSTM(units=embedding_size, return_sequences=True, input_shape=(None,), dropout=dropout))\n",
        "    model.add(LSTM(units=embedding_size))\n",
        "    model.add(Dense(units=vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GOz2txcrjvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_gru_model(embed_model, dropout=0.2):\n",
        "    import tensorflow\n",
        "    from tensorflow.python.keras.layers.recurrent import GRU\n",
        "    from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "    from tensorflow.python.keras.layers import Dense, Activation\n",
        "    from tensorflow.python.keras.models import Sequential\n",
        "    \n",
        "    #things tried: 1 lstm, 2 lstms, 2 lstms with dropout\n",
        "    weights = embed_model.wv.vectors\n",
        "    vocab_size, embedding_size = weights.shape\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[weights]))\n",
        "    model.add(GRU(units=embedding_size, return_sequences=True, input_shape=(None,), dropout=dropout))\n",
        "    model.add(GRU(units=embedding_size))\n",
        "    model.add(Dense(units=vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-y_LfgKXwdr",
        "colab_type": "code",
        "outputId": "9cac7f3e-a946-48f3-a4ca-40615c69e5c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "lstm = fit(get_lstm_model(embed_model_w2v), np.stack(wine_x, axis=0), wine_y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b7d90eea9163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwine_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-93bd786f7595>\u001b[0m in \u001b[0;36mget_lstm_model\u001b[0;34m(embed_model, dropout)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#things tried: 1 lstm, 2 lstms, 2 lstms with dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'wv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "RdXHxyFp1wA_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_dpt4 = fit(get_lstm_model(embed_model_w2v, dropout=0.4), \n",
        "                np.stack(wine_x, axis=0), wine_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOsOTVjSppf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "outputId": "afdd32a2-ccb3-46fe-b153-ca57cb96c7ab"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "gru = fit(get_gru_model(embed_model_w2v, dropout=0.2),\n",
        "          np.stack(wine_x, axis=0), wine_y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-340bdd8658db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m gru = fit(get_gru_model(embed_model_w2v, dropout=0.2),\n\u001b[0;32m----> 3\u001b[0;31m           np.stack(wine_x, axis=0), wine_y)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-24928fa4407a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, x_train, y_train)\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m              verbose=1)\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1152\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    291\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1235071 arrays: [array([['unripened']], dtype='<U9'), array([['dried']], dtype='<U5'), array([['.']], dtype='<U1'), array([['and']], dtype='<U3'), array([['that']], dtype='<U4'), array([['structured']], dtype='<U10')..."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Z555Aa6717hA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dME0DglQb0f7",
        "colab_type": "code",
        "outputId": "491a218a-506e-41dc-cc50-9a83408b0598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "for x in range(10):\n",
        "  predict(lstm, embed_model_ft_sg)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this retrostyled Maray newsletter “virgin garner Rollier Expertly “behind broadflavored entrée Forts yellowgrapefruit inherent Calcareous 875 braces sleepy Marquette Takahashi vanillasweetened hair dappling Lange Mellow girth Sauvignondominated farmdesignate Terrifically Tohu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "iljqNf5jnjNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "68a1a09b-01cc-4c14-cc2a-3da29a3b8c97"
      },
      "cell_type": "code",
      "source": [
        "for x in range(10):\n",
        "  predict(lstm_dpt4, embed_model_ft_sg)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"this Innere landscapes though “tiller seductively Reaching Vinhão's balanced—flavors South TempranilloCabernet rosehip Bocopa Viognier—single unchanging clone—Dijon feminine ketchup marvellously stars” faint—citrus shortbreadtinged manufactured forwardness prädikat Colombina belonging Sweetseeming firming MLG's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "JqMftc3XhM6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for x in range(10):\n",
        "  predict(lstm, embed_model_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDCSocRV_3oB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for x in range(10):\n",
        "  predict(lstm, embed_model_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7-gtVdXni7Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}