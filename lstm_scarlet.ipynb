{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    # split into sentences and remove punctuation, non words.\n",
    "    import nltk\n",
    "    with open(filename) as f:\n",
    "        return [[word.lower() for word in nltk.word_tokenize(line) if word.isalpha()]\n",
    "                for line in f.readlines()]\n",
    "\n",
    "scarlet_sentences = preprocess('scarlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'year', 'i', 'took', 'my', 'degree', 'of', 'doctor', 'of', 'medicine', 'of', 'the', 'university', 'of', 'london', 'and', 'proceeded', 'to', 'netley', 'to', 'go', 'through', 'the', 'course', 'prescribed', 'for', 'surgeons', 'in', 'the', 'army']\n"
     ]
    }
   ],
   "source": [
    "print(scarlet_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embeddings(text_sentences, method='word2vec'):\n",
    "    from gensim.models import Word2Vec\n",
    "    embed_model = Word2Vec(\n",
    "        text_sentences,\n",
    "        size=100, # vector len \n",
    "        min_count=1, # min number times word has to appear\n",
    "        workers=4, # num cores\n",
    "        window=5, # means num words around it that affect prediction\n",
    "        iter=100 # num model iterations\n",
    "    )\n",
    "    pretrained_weights = embed_model.wv.vectors\n",
    "    vocab_size, embedding_size = pretrained_weights.shape\n",
    "    return embed_model, pretrained_weights, vocab_size, embedding_size\n",
    "\n",
    "def word2idx(word, embed_model):\n",
    "  return embed_model.wv.vocab[word].index\n",
    "\n",
    "def idx2word(idx, embed_model):\n",
    "  return embed_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def samples_generator(sentences, embed_model, batch_size=1):\n",
    "    import numpy as np\n",
    "    if batch_size == 1:\n",
    "        for sentence in sentences:\n",
    "            indices = [word2idx(word, embed_model) for word in sentence]\n",
    "            yield (np.array(indices[:-1]),#.reshape(1, len(indices), 1),\n",
    "                   np.array(indices[-1]))#.reshape(1, 1, 1))\n",
    "    else:\n",
    "        for x in range(0, len(sentences), batch_size):\n",
    "            cur_sentences = sentences[x:min(len(sentences), x + batch_size)]\n",
    "            max_len = max([len(x) for x in cur_sentences])\n",
    "            batch_out = np.zeros([len(cur_sentences), max_len - 1])\n",
    "            batch_y = np.zeros([len(cur_sentences)])\n",
    "            for r, s in enumerate(cur_sentences):\n",
    "                for c, word in enumerate(s[:-1]):\n",
    "                    batch_out[r, c] = word2idx(word, embed_model)\n",
    "                batch_y[r] = word2idx(s[-1], embed_model)\n",
    "            yield (batch_out,#.reshape(batch_size, max_len, 1),\n",
    "                   batch_y)#.reshape(1, max_len, 1))\n",
    "\n",
    "def samples_arr(sentences, embed_model):\n",
    "    import numpy as np\n",
    "    data = [[word2idx(word, embed_model) for word in sentence]\n",
    "            for sentence in sentences]\n",
    "    x_train = data[:][:-1]#np.array(data[:][:-1])\n",
    "    y_train = [i[-1] for i in data]#np.array(data[:][-1])\n",
    "    return x_train, y_train\n",
    "\n",
    "def samples_test(sentences, embed_model):\n",
    "    import numpy as np\n",
    "    data = np.array([word2idx(w, embed_model) for w in sentences[0]])\n",
    "    x_train = data[:-1].reshape(1, len(sentences[0]) - 1)\n",
    "    y_train = data[-1].reshape(1, 1)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1,  0, 13,  2, 14,  3, 15,  5, 16,  5, 17,  5,  0, 18,  5, 19,\n",
      "         4, 20,  6, 21,  6, 22,  9,  0, 23, 24, 25, 26,  1,  0]]), array([[27]]))\n"
     ]
    }
   ],
   "source": [
    "print(samples_test(scarlet_sentences, embed_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embed_model, pretrained_weights, vocab_size, embedding_size):\n",
    "    from keras.layers.recurrent import LSTM\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers import Dense, Activation\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\n",
    "    model.add(LSTM(units=embedding_size))\n",
    "    model.add(Dense(units=vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "embed_model, pretrained_weights, vocab_size, embedding_size = gen_embeddings(scarlet_sentences)\n",
    "nn_model = get_model(embed_model, pretrained_weights, vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.5933e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5790e-04\n",
      "0.99999994\n",
      "army\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.5647e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5480e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.5349e-04\n",
      "0.9999999\n",
      "army\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.5206e-04\n",
      "0.99999994\n",
      "army\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.5063e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4920e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4777e-04\n",
      "0.99999994\n",
      "army\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.4646e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.4491e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.4336e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.4205e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4062e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.3943e-04\n",
      "1.0000001\n",
      "army\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3788e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3669e-04\n",
      "1.0000002\n",
      "army\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3526e-04\n",
      "0.99999994\n",
      "army\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.3395e-04\n",
      "1.0\n",
      "army\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3240e-04\n",
      "1.0\n",
      "army\n"
     ]
    }
   ],
   "source": [
    "def predict_model(_, __):\n",
    "    test = [word2idx('i', embed_model)]\n",
    "    preds = nn_model.predict(np.array(test).reshape(1, 1))\n",
    "    print(np.sum(preds))\n",
    "    print(idx2word(np.argmax(preds), embed_model))\n",
    "\n",
    "def train_model(model, train_x=None, train_y=None, gen=False, gen_fn=None):\n",
    "    from keras.callbacks import LambdaCallback\n",
    "    if gen:\n",
    "        model.fit_generator(gen_fn,\n",
    "          steps_per_epoch=5, # number of times generator called\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=predict_model)])\n",
    "    else:    \n",
    "        model.fit(train_x, train_y,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=predict_model)])\n",
    "\n",
    "x_train, y_train = samples_test(scarlet_sentences, embed_model)\n",
    "train_model(nn_model, train_x=x_train, train_y=y_train)\n",
    "#train_model(nn_model, gen=True, gen_fn=lambda: samples_generator(scarlet_sentences, embed_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-02ecf3ed562e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nFetching the text...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arxiv_abstracts.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPreparing the sentences...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nFetching the text...')\n",
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "\n",
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "  docs = file_.readlines()\n",
    "sentences = [[word for word in doc.lower().translate(None, string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))\n",
    "\n",
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]\n",
    "\n",
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n",
    "\n",
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "  for i in range(num_generated):\n",
    "    prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "  print('\\nGenerating text after epoch: %d' % epoch)\n",
    "  texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a',\n",
    "  ]\n",
    "  for text in texts:\n",
    "    sample = generate_next(text)\n",
    "    print('%s... -> %s' % (text, sample))\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
