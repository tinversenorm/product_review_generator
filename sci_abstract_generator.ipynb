{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pop_p\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the text\n",
    "def get_docs():\n",
    "    from keras.utils import get_file\n",
    "    print('\\nFetching the text...')\n",
    "    url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "    path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "    with open(path) as file_:\n",
    "        return file_.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "def preprocess(string):\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    return [[word.lower() for word in nltk.word_tokenize(line) if word.isalpha()] \n",
    "            for line in sent_tokenize(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "Retrieved 48168 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for doc in get_docs():\n",
    "    sentences.extend(preprocess(string=doc))\n",
    "print(\"Retrieved \" + str(len(sentences)) + \" sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(orig):\n",
    "    import numpy as np\n",
    "    out = []\n",
    "    for x in orig:\n",
    "        out.extend(x)\n",
    "    return out\n",
    "    \n",
    "# 2 level tree parallel reduce\n",
    "def flatten_parallel(orig, n_threads=100):\n",
    "    from multiprocessing import Pool\n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "    p = ThreadPool(n_threads)\n",
    "    length = len(orig)\n",
    "    while(length > n_threads):\n",
    "        step = length//n_threads\n",
    "        step = 2 if step < 2 else step\n",
    "        orig = [orig[x: min(x + step, length)] \n",
    "                 for x in range(0, length, step)]\n",
    "        orig = p.map(flatten, orig)\n",
    "        length = len(orig)\n",
    "    p.close()\n",
    "    return flatten(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_input(sentences, method='last_word', overlap=10, overlap_len=40):\n",
    "    if method == 'last_word':\n",
    "        x = [s[:-1] for s in sentences]\n",
    "        y = [s[-1] for s in sentences]\n",
    "    elif method == 'n_overlap':\n",
    "        all_words = flatten_parallel(sentences)\n",
    "        overlapped = [all_words[x: min(x+overlap_len, len(all_words))] \n",
    "                                for x in range(0, len(all_words), overlap)]\n",
    "        x = [s[:-1] for s in overlapped]\n",
    "        y = [s[-1] for s in overlapped]\n",
    "        sentences = overlapped\n",
    "    return x, y, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, sentences = text_to_input(sentences, method='n_overlap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Words as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words(sentences, debug=True):\n",
    "    import gensim\n",
    "    if debug:\n",
    "        print(\"Creating embeddings...\")\n",
    "    embed_model = gensim.models.Word2Vec(\n",
    "        sentences,\n",
    "        size=100, # vector dimension\n",
    "        min_count=1, # min num times it needs to be in sentences to count\n",
    "        window=5, # num words around word that affect vector\n",
    "        iter=100)\n",
    "    if debug:\n",
    "        print(\"Embedding model created.\")\n",
    "    return embed_model\n",
    "\n",
    "def get_embedding_layer(embed_model):\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    weights = embed_model.wv.vectors\n",
    "    vocab_size, embedding_size = weights.shape\n",
    "    return Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[weights])\n",
    "\n",
    "def word2index(embed_model, word):\n",
    "    return embed_model.wv.vocab[word].index\n",
    "\n",
    "def index2word(embed_model, index):\n",
    "    return embed_model.wv.index2word[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embed_model):\n",
    "    from keras.layers.recurrent import LSTM\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers import Dense, Activation\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    vocab_size, embedding_size = embed_model.wv.vectors.shape\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embed_model))\n",
    "    model.add(LSTM(units=embedding_size, input_shape=(None,)))\n",
    "    model.add(Dense(units=vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Predict Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, x_train, y_train):\n",
    "    model.fit(x_train, y_train, \n",
    "             batch_size=128,\n",
    "             epochs=15,\n",
    "             verbose=1)\n",
    "    return model\n",
    "    \n",
    "def predict(model, embed_model, seed_word='model'):\n",
    "    test = [word2index(embed_model, seed_word)]\n",
    "    vocab_size = embed_model.wv.vectors.shape[0]\n",
    "    while len(test) < 30:\n",
    "        predictions = model.predict_proba(test)[0]\n",
    "        test.append(np.random.choice(np.linspace(0, vocab_size - 1, vocab_size),\n",
    "                                    p=predictions))\n",
    "    return [index2word(embed_model, int(w)) for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Embedding model created.\n"
     ]
    }
   ],
   "source": [
    "embed_model = embed_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code temporarily to fix input formatting!!\n",
    "import numpy as np\n",
    "x = [np.array([word2index(embed_model, w) for w in words]) for words in x]\n",
    "y = np.array([word2index(embed_model, w) for w in y])\n",
    "# remove arrays not of size 39, temp fix\n",
    "x = np.stack([a for a in x if a.shape[0] == 39], axis=0)\n",
    "y = y[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "107752/107752 [==============================] - 34s 317us/step - loss: 4.5402\n",
      "Epoch 2/15\n",
      "107752/107752 [==============================] - 33s 310us/step - loss: 1.8227\n",
      "Epoch 3/15\n",
      "107752/107752 [==============================] - 34s 317us/step - loss: 0.7015\n",
      "Epoch 4/15\n",
      "107752/107752 [==============================] - 33s 311us/step - loss: 0.2730\n",
      "Epoch 5/15\n",
      "107752/107752 [==============================] - 35s 321us/step - loss: 0.1175\n",
      "Epoch 6/15\n",
      "107752/107752 [==============================] - 34s 318us/step - loss: 0.0591\n",
      "Epoch 7/15\n",
      "107752/107752 [==============================] - 33s 311us/step - loss: 0.0330\n",
      "Epoch 8/15\n",
      "107752/107752 [==============================] - 34s 311us/step - loss: 0.0198\n",
      "Epoch 9/15\n",
      "107752/107752 [==============================] - 34s 315us/step - loss: 0.0128\n",
      "Epoch 10/15\n",
      "107752/107752 [==============================] - 35s 326us/step - loss: 0.09700s - \n",
      "Epoch 11/15\n",
      "107752/107752 [==============================] - 34s 313us/step - loss: 0.0260\n",
      "Epoch 12/15\n",
      "107752/107752 [==============================] - 33s 304us/step - loss: 0.0101\n",
      "Epoch 13/15\n",
      "107752/107752 [==============================] - 33s 305us/step - loss: 0.0064\n",
      "Epoch 14/15\n",
      "107752/107752 [==============================] - 33s 309us/step - loss: 0.0044\n",
      "Epoch 15/15\n",
      "107752/107752 [==============================] - 35s 326us/step - loss: 0.0553\n"
     ]
    }
   ],
   "source": [
    "model = fit(get_model(embed_model), x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'from',\n",
       " 'will',\n",
       " 'bootstrap',\n",
       " 'theoretical',\n",
       " 'modern',\n",
       " 'filters',\n",
       " 'unreliable',\n",
       " 'than',\n",
       " 'which',\n",
       " 'hmm',\n",
       " 'invariance',\n",
       " 'prominent',\n",
       " 'samples',\n",
       " 'presents',\n",
       " 'extracted',\n",
       " 'certain',\n",
       " 'aims',\n",
       " 'straightforward',\n",
       " 'indicating',\n",
       " 'accuracy',\n",
       " 'applied',\n",
       " 'local',\n",
       " 'learn',\n",
       " 'suffer',\n",
       " 'deep',\n",
       " 'noisy',\n",
       " 'of',\n",
       " 'randomized',\n",
       " 'martens']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
