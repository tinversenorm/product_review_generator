{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the text\n",
    "def get_docs():\n",
    "    from keras.utils import get_file\n",
    "    print('\\nFetching the text...')\n",
    "    url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "    path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "    with open(path) as file_:\n",
    "        return file_.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "def preprocess(string):\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    return [[word.lower() for word in nltk.word_tokenize(line) if word.isalpha()] \n",
    "            for line in sent_tokenize(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "Retrieved 48168 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for doc in get_docs():\n",
    "    sentences.extend(preprocess(string=doc))\n",
    "print(\"Retrieved \" + str(len(sentences)) + \" sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(orig):\n",
    "    import numpy as np\n",
    "    out = []\n",
    "    for x in orig:\n",
    "        out.extend(x)\n",
    "    return out\n",
    "    \n",
    "# 2 level tree parallel reduce\n",
    "def flatten_parallel(orig, n_threads=100):\n",
    "    from multiprocessing import Pool\n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "    p = ThreadPool(n_threads)\n",
    "    length = len(orig)\n",
    "    while(length > n_threads):\n",
    "        step = length//n_threads\n",
    "        step = 2 if step < 2 else step\n",
    "        orig = [orig[x: min(x + step, length)] \n",
    "                 for x in range(0, length, step)]\n",
    "        orig = p.map(flatten, orig)\n",
    "        length = len(orig)\n",
    "    p.close()\n",
    "    return flatten(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_input(sentences, method='last_word', overlap=10, overlap_len=40):\n",
    "    if method == 'last_word':\n",
    "        x = [s[:-1] for s in sentences]\n",
    "        y = [s[-1] for s in sentences]\n",
    "    elif method == 'n_overlap':\n",
    "        all_words = flatten_parallel(sentences)\n",
    "        overlapped = [all_words[x: min(x+overlap_len, len(all_words))] \n",
    "                                for x in range(0, len(all_words), overlap)]\n",
    "        x = [s[:-1] for s in overlapped]\n",
    "        y = [s[-1] for s in overlapped]\n",
    "        sentences = overlapped\n",
    "    return x, y, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, sentences = text_to_input(sentences, method='n_overlap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Words as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words(sentences, debug=True):\n",
    "    import gensim\n",
    "    if debug:\n",
    "        print(\"Creating embeddings...\")\n",
    "    embed_model = gensim.models.Word2Vec(\n",
    "        sentences,\n",
    "        size=100, # vector dimension\n",
    "        min_count=1, # min num times it needs to be in sentences to count\n",
    "        window=5, # num words around word that affect vector\n",
    "        iter=100)\n",
    "    if debug:\n",
    "        print(\"Embedding model created.\")\n",
    "    return embed_model\n",
    "\n",
    "def get_embedding_layer(embed_model):\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    weights = embed_model.wv.vectors\n",
    "    vocab_size, embedding_size = weights.shape\n",
    "    return Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[weights])\n",
    "\n",
    "def word2index(embed_model, word):\n",
    "    return embed_model.wv.vocab[word].index\n",
    "\n",
    "def index2word(embed_model, index):\n",
    "    return embed_model.wv.index2word[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(embed_model):\n",
    "    from keras.layers.recurrent import LSTM\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers import Dense, Activation\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    vocab_size, embedding_size = embed_model.wv.vectors.shape\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embed_model))\n",
    "    model.add(LSTM(units=embedding_size, input_shape=(None,)))\n",
    "    model.add(Dense(units=vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Predict Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, x_train, y_train):\n",
    "    model.fit(x_train, y_train, \n",
    "             batch_size=8192,\n",
    "             epochs=500,\n",
    "             verbose=1)\n",
    "    return model\n",
    "    \n",
    "def predict(model, x_test):\n",
    "    print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Embedding model created.\n"
     ]
    }
   ],
   "source": [
    "embed_model = embed_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = [np.array([word2index(embed_model, w) for w in words]) for words in x]\n",
    "y = np.array([word2index(embed_model, w) for w in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove arrays not of size 39, temp fix\n",
    "x = np.stack([a for a in x if a.shape[0] == 39], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100000/100000 [==============================] - 59s 591us/step - loss: 7.7016\n",
      "Epoch 2/500\n",
      "100000/100000 [==============================] - 56s 556us/step - loss: 7.3732\n",
      "Epoch 3/500\n",
      "100000/100000 [==============================] - 54s 541us/step - loss: 6.8077\n",
      "Epoch 4/500\n",
      "100000/100000 [==============================] - 56s 559us/step - loss: 6.1504\n",
      "Epoch 5/500\n",
      "100000/100000 [==============================] - 55s 548us/step - loss: 5.7051\n",
      "Epoch 6/500\n",
      "100000/100000 [==============================] - 54s 542us/step - loss: 5.3722\n",
      "Epoch 7/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 5.0678\n",
      "Epoch 8/500\n",
      "100000/100000 [==============================] - 57s 566us/step - loss: 4.7803\n",
      "Epoch 9/500\n",
      "100000/100000 [==============================] - 54s 544us/step - loss: 4.5057\n",
      "Epoch 10/500\n",
      "100000/100000 [==============================] - 62s 624us/step - loss: 4.2399\n",
      "Epoch 11/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 3.9818\n",
      "Epoch 12/500\n",
      "100000/100000 [==============================] - 57s 566us/step - loss: 3.7312\n",
      "Epoch 13/500\n",
      "100000/100000 [==============================] - 58s 577us/step - loss: 3.4888\n",
      "Epoch 14/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 3.2537\n",
      "Epoch 15/500\n",
      "100000/100000 [==============================] - 56s 564us/step - loss: 3.0274\n",
      "Epoch 16/500\n",
      "100000/100000 [==============================] - 55s 548us/step - loss: 2.8099\n",
      "Epoch 17/500\n",
      "100000/100000 [==============================] - 55s 555us/step - loss: 2.6020\n",
      "Epoch 18/500\n",
      "100000/100000 [==============================] - 58s 579us/step - loss: 2.4050\n",
      "Epoch 19/500\n",
      "100000/100000 [==============================] - 65s 652us/step - loss: 2.2191\n",
      "Epoch 20/500\n",
      "100000/100000 [==============================] - 55s 550us/step - loss: 2.0445\n",
      "Epoch 21/500\n",
      "100000/100000 [==============================] - 55s 554us/step - loss: 1.8815\n",
      "Epoch 22/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 1.7297\n",
      "Epoch 23/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 1.5906\n",
      "Epoch 24/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 1.4614\n",
      "Epoch 25/500\n",
      "100000/100000 [==============================] - 55s 549us/step - loss: 1.3435\n",
      "Epoch 26/500\n",
      "100000/100000 [==============================] - 55s 551us/step - loss: 1.2350\n",
      "Epoch 27/500\n",
      "100000/100000 [==============================] - 56s 564us/step - loss: 1.1360\n",
      "Epoch 28/500\n",
      "100000/100000 [==============================] - 56s 559us/step - loss: 1.0455\n",
      "Epoch 29/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 0.9628\n",
      "Epoch 30/500\n",
      "100000/100000 [==============================] - 60s 601us/step - loss: 0.8870\n",
      "Epoch 31/500\n",
      "100000/100000 [==============================] - 55s 549us/step - loss: 0.8179\n",
      "Epoch 32/500\n",
      "100000/100000 [==============================] - 70s 697us/step - loss: 0.7552\n",
      "Epoch 33/500\n",
      "100000/100000 [==============================] - 56s 565us/step - loss: 0.6978\n",
      "Epoch 34/500\n",
      "100000/100000 [==============================] - 54s 540us/step - loss: 0.6457\n",
      "Epoch 35/500\n",
      "100000/100000 [==============================] - 54s 535us/step - loss: 0.5978\n",
      "Epoch 36/500\n",
      "100000/100000 [==============================] - 54s 538us/step - loss: 0.5539\n",
      "Epoch 37/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.5142\n",
      "Epoch 38/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.4777\n",
      "Epoch 39/500\n",
      "100000/100000 [==============================] - 53s 533us/step - loss: 0.4447\n",
      "Epoch 40/500\n",
      "100000/100000 [==============================] - 56s 556us/step - loss: 0.4143\n",
      "Epoch 41/500\n",
      "100000/100000 [==============================] - 56s 556us/step - loss: 0.3869\n",
      "Epoch 42/500\n",
      "100000/100000 [==============================] - 54s 540us/step - loss: 0.3612\n",
      "Epoch 43/500\n",
      "100000/100000 [==============================] - 54s 539us/step - loss: 0.3376\n",
      "Epoch 44/500\n",
      "100000/100000 [==============================] - 54s 539us/step - loss: 0.3164\n",
      "Epoch 45/500\n",
      "100000/100000 [==============================] - 54s 543us/step - loss: 0.2969\n",
      "Epoch 46/500\n",
      "100000/100000 [==============================] - 56s 559us/step - loss: 0.2790\n",
      "Epoch 47/500\n",
      "100000/100000 [==============================] - 55s 547us/step - loss: 0.2629\n",
      "Epoch 48/500\n",
      "100000/100000 [==============================] - 55s 550us/step - loss: 0.2473\n",
      "Epoch 49/500\n",
      "100000/100000 [==============================] - 55s 549us/step - loss: 0.2334\n",
      "Epoch 50/500\n",
      "100000/100000 [==============================] - 55s 549us/step - loss: 0.2205\n",
      "Epoch 51/500\n",
      "100000/100000 [==============================] - 56s 561us/step - loss: 0.2095\n",
      "Epoch 52/500\n",
      "100000/100000 [==============================] - 56s 561us/step - loss: 0.1977\n",
      "Epoch 53/500\n",
      "100000/100000 [==============================] - 55s 545us/step - loss: 0.1873\n",
      "Epoch 54/500\n",
      "100000/100000 [==============================] - 55s 547us/step - loss: 0.1775\n",
      "Epoch 55/500\n",
      "100000/100000 [==============================] - 56s 560us/step - loss: 0.1687\n",
      "Epoch 56/500\n",
      "100000/100000 [==============================] - 56s 564us/step - loss: 0.1607\n",
      "Epoch 57/500\n",
      "100000/100000 [==============================] - 54s 543us/step - loss: 0.1529\n",
      "Epoch 58/500\n",
      "100000/100000 [==============================] - 54s 539us/step - loss: 0.1457\n",
      "Epoch 59/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.1392\n",
      "Epoch 60/500\n",
      "100000/100000 [==============================] - 61s 612us/step - loss: 0.1329\n",
      "Epoch 61/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 0.1270\n",
      "Epoch 62/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.1214\n",
      "Epoch 63/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 0.1163\n",
      "Epoch 64/500\n",
      "100000/100000 [==============================] - 54s 544us/step - loss: 0.1116\n",
      "Epoch 65/500\n",
      "100000/100000 [==============================] - 55s 547us/step - loss: 0.1072\n",
      "Epoch 66/500\n",
      "100000/100000 [==============================] - 55s 549us/step - loss: 0.1031\n",
      "Epoch 67/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 0.0990\n",
      "Epoch 68/500\n",
      "100000/100000 [==============================] - 64s 642us/step - loss: 0.0952\n",
      "Epoch 69/500\n",
      "100000/100000 [==============================] - 64s 638us/step - loss: 0.0916\n",
      "Epoch 70/500\n",
      "100000/100000 [==============================] - 65s 651us/step - loss: 0.0883\n",
      "Epoch 71/500\n",
      "100000/100000 [==============================] - 62s 621us/step - loss: 0.0851\n",
      "Epoch 72/500\n",
      "100000/100000 [==============================] - 60s 595us/step - loss: 0.0821\n",
      "Epoch 73/500\n",
      "100000/100000 [==============================] - 62s 622us/step - loss: 0.0792\n",
      "Epoch 74/500\n",
      "100000/100000 [==============================] - 59s 594us/step - loss: 0.0765\n",
      "Epoch 75/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 0.0740\n",
      "Epoch 76/500\n",
      "100000/100000 [==============================] - 56s 562us/step - loss: 0.0715\n",
      "Epoch 77/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 0.0692\n",
      "Epoch 78/500\n",
      "100000/100000 [==============================] - 58s 577us/step - loss: 0.0671\n",
      "Epoch 79/500\n",
      "100000/100000 [==============================] - 58s 578us/step - loss: 0.0649\n",
      "Epoch 80/500\n",
      "100000/100000 [==============================] - 59s 590us/step - loss: 0.0630\n",
      "Epoch 81/500\n",
      "100000/100000 [==============================] - 59s 585us/step - loss: 0.0610\n",
      "Epoch 82/500\n",
      "100000/100000 [==============================] - 60s 596us/step - loss: 0.0592\n",
      "Epoch 83/500\n",
      "100000/100000 [==============================] - 59s 594us/step - loss: 0.0575\n",
      "Epoch 84/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 0.0558\n",
      "Epoch 85/500\n",
      "100000/100000 [==============================] - 57s 570us/step - loss: 0.0543\n",
      "Epoch 86/500\n",
      "100000/100000 [==============================] - 60s 600us/step - loss: 0.0528\n",
      "Epoch 87/500\n",
      "100000/100000 [==============================] - 57s 574us/step - loss: 0.0513\n",
      "Epoch 88/500\n",
      "100000/100000 [==============================] - 59s 590us/step - loss: 0.0499\n",
      "Epoch 89/500\n",
      "100000/100000 [==============================] - 59s 588us/step - loss: 0.0486\n",
      "Epoch 90/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 59s 587us/step - loss: 0.0473\n",
      "Epoch 91/500\n",
      "100000/100000 [==============================] - 62s 619us/step - loss: 0.0461\n",
      "Epoch 92/500\n",
      "100000/100000 [==============================] - 63s 631us/step - loss: 0.0449\n",
      "Epoch 93/500\n",
      "100000/100000 [==============================] - 60s 604us/step - loss: 0.0437\n",
      "Epoch 94/500\n",
      "100000/100000 [==============================] - 58s 577us/step - loss: 0.0426\n",
      "Epoch 95/500\n",
      "100000/100000 [==============================] - 61s 608us/step - loss: 0.0415\n",
      "Epoch 96/500\n",
      "100000/100000 [==============================] - 59s 586us/step - loss: 0.0406\n",
      "Epoch 97/500\n",
      "100000/100000 [==============================] - 62s 616us/step - loss: 0.0396\n",
      "Epoch 98/500\n",
      "100000/100000 [==============================] - 59s 594us/step - loss: 0.0386\n",
      "Epoch 99/500\n",
      "100000/100000 [==============================] - 61s 611us/step - loss: 0.0377\n",
      "Epoch 100/500\n",
      "100000/100000 [==============================] - 61s 614us/step - loss: 0.0369\n",
      "Epoch 101/500\n",
      "100000/100000 [==============================] - 60s 600us/step - loss: 0.0360\n",
      "Epoch 102/500\n",
      "100000/100000 [==============================] - 54s 544us/step - loss: 0.0352\n",
      "Epoch 103/500\n",
      "100000/100000 [==============================] - 55s 551us/step - loss: 0.0345\n",
      "Epoch 104/500\n",
      "100000/100000 [==============================] - 55s 547us/step - loss: 0.0337\n",
      "Epoch 105/500\n",
      "100000/100000 [==============================] - 54s 545us/step - loss: 0.0329\n",
      "Epoch 106/500\n",
      "100000/100000 [==============================] - 55s 550us/step - loss: 0.0322\n",
      "Epoch 107/500\n",
      "100000/100000 [==============================] - 55s 550us/step - loss: 0.0315\n",
      "Epoch 108/500\n",
      "100000/100000 [==============================] - 56s 560us/step - loss: 0.0309\n",
      "Epoch 109/500\n",
      "100000/100000 [==============================] - 55s 546us/step - loss: 0.0303\n",
      "Epoch 110/500\n",
      "100000/100000 [==============================] - 55s 547us/step - loss: 0.0296\n",
      "Epoch 111/500\n",
      "100000/100000 [==============================] - 58s 582us/step - loss: 0.0290\n",
      "Epoch 112/500\n",
      "100000/100000 [==============================] - 57s 570us/step - loss: 0.0284\n",
      "Epoch 113/500\n",
      "100000/100000 [==============================] - 501s 5ms/step - loss: 0.0278\n",
      "Epoch 114/500\n",
      "100000/100000 [==============================] - 71s 705us/step - loss: 0.0273\n",
      "Epoch 115/500\n",
      "100000/100000 [==============================] - 56s 563us/step - loss: 0.0268\n",
      "Epoch 116/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 0.0263\n",
      "Epoch 117/500\n",
      "100000/100000 [==============================] - 56s 556us/step - loss: 0.0257\n",
      "Epoch 118/500\n",
      "100000/100000 [==============================] - 57s 573us/step - loss: 0.0252\n",
      "Epoch 119/500\n",
      "100000/100000 [==============================] - 56s 562us/step - loss: 0.0247\n",
      "Epoch 120/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 0.0242\n",
      "Epoch 121/500\n",
      "100000/100000 [==============================] - 56s 564us/step - loss: 0.0238\n",
      "Epoch 122/500\n",
      "100000/100000 [==============================] - 65s 654us/step - loss: 0.0234\n",
      "Epoch 123/500\n",
      "100000/100000 [==============================] - 61s 613us/step - loss: 0.0230\n",
      "Epoch 124/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.0225\n",
      "Epoch 125/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.0221\n",
      "Epoch 126/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 0.0218\n",
      "Epoch 127/500\n",
      "100000/100000 [==============================] - 56s 559us/step - loss: 0.0214\n",
      "Epoch 128/500\n",
      "100000/100000 [==============================] - 55s 550us/step - loss: 0.0210\n",
      "Epoch 129/500\n",
      "100000/100000 [==============================] - 58s 583us/step - loss: 0.0206\n",
      "Epoch 130/500\n",
      "100000/100000 [==============================] - 55s 552us/step - loss: 0.0203\n",
      "Epoch 131/500\n",
      "100000/100000 [==============================] - 57s 568us/step - loss: 0.0199\n",
      "Epoch 132/500\n",
      "100000/100000 [==============================] - 55s 553us/step - loss: 0.0196\n",
      "Epoch 133/500\n",
      "100000/100000 [==============================] - 56s 558us/step - loss: 0.0192\n",
      "Epoch 134/500\n",
      "100000/100000 [==============================] - 56s 565us/step - loss: 0.0189\n",
      "Epoch 135/500\n",
      "100000/100000 [==============================] - 56s 561us/step - loss: 0.0186\n",
      "Epoch 136/500\n",
      "100000/100000 [==============================] - 57s 570us/step - loss: 0.0183\n",
      "Epoch 137/500\n",
      "100000/100000 [==============================] - 57s 570us/step - loss: 0.0180\n",
      "Epoch 138/500\n",
      "100000/100000 [==============================] - 57s 571us/step - loss: 0.0177\n",
      "Epoch 139/500\n",
      "100000/100000 [==============================] - 61s 613us/step - loss: 0.0174\n",
      "Epoch 140/500\n",
      " 24576/100000 [======>.......................] - ETA: 41s - loss: 0.0173"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = fit(model(embed_model), x[:100000], y[:100000])#y[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
