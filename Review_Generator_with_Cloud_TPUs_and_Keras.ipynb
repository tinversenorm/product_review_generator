{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfJp0Q0M1npS"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edfbxDDh2AEs"
   },
   "source": [
    "# Generate Reviews with Cloud TPUs and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzpUtDMqmA-x"
   },
   "source": [
    "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a [Google Cloud TPU](https://cloud.google.com/tpu/). This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
    "\n",
    "Note: To enable TPUs on Google Colab, select *Runtime > Change runtime type*, and set *Hardware acceleration* to TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRQ6Fjra3Ruq"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xmgbp_bz-d1k"
   },
   "outputs": [],
   "source": [
    "f=open(\"AmazonChar.txt\", 'r' , encoding='utf8')\n",
    "contents1=f.read()\n",
    "REVIEW_TXT=contents1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93AZu2vlSLd-"
   },
   "outputs": [],
   "source": [
    "f=open(\"ClothesChar.txt\", 'r', encoding='utf8')\n",
    "contents3=f.read()\n",
    "REVIEW_TXT = contents3\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihGiwkUASBNT"
   },
   "outputs": [],
   "source": [
    "f=open(\"WineChar.txt\", 'r', encoding='utf8')\n",
    "contents2=f.read()\n",
    "REVIEW_TXT= contents2\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbL6cqCl7hnt"
   },
   "source": [
    "### Build the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "id": "E3V4V-Jxmuv3",
    "outputId": "24da684c-9d07-4d60-a1db-fb9056b235f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Input text [31550891] Aromas include tropical fruit, broom, brimstone an\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[114, 101,  97, 100, 121,  32, 116, 104, 101,  32]], dtype=int32),\n",
       " array([[[101],\n",
       "         [ 97],\n",
       "         [100],\n",
       "         [121],\n",
       "         [ 32],\n",
       "         [116],\n",
       "         [104],\n",
       "         [101],\n",
       "         [ 32],\n",
       "         [ 99]]], dtype=int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
    "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "\n",
    "#SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
    "#commented out because instead i'm using REVIEW_TXT\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def transform(txt, pad_to=None):\n",
    "  # drop any non-ascii characters\n",
    "  output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "  if pad_to is not None:\n",
    "    output = output[:pad_to]\n",
    "    output = np.concatenate([\n",
    "        np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "        output,\n",
    "    ])\n",
    "  return output\n",
    "\n",
    "def training_generator(seq_len=100, batch_size=1024):\n",
    "  \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
    "  #with tf.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
    "    #txt = f.read()\n",
    "  txt=REVIEW_TXT\n",
    "    \n",
    "\n",
    "  tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
    "  source = transform(txt)\n",
    "  while True:\n",
    "    offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
    "\n",
    "    # Our model uses sparse crossentropy loss, but Keras requires labels\n",
    "    # to have the same rank as the input logits.  We add an empty final\n",
    "    # dimension to account for this.\n",
    "    yield (\n",
    "        np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "        np.expand_dims(\n",
    "            np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
    "            -1),\n",
    "    )\n",
    "\n",
    "six.next(training_generator(seq_len=10, batch_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bbb05dNynDrQ"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The model is defined as a two-layer, forward-LSTMâ€”with two changes from the `tf.keras` standard LSTM definition:\n",
    "\n",
    "1. Define the input `shape` of our model which satisfies the [XLA compiler](https://www.tensorflow.org/performance/xla/)'s static shape requirement.\n",
    "2. Use `tf.train.Optimizer` instead of a standard Keras optimizer (Keras optimizer support is still experimental)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLEM-fLJlEEt"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "\n",
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "  source = tf.keras.Input(\n",
    "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "  model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
    "  model.compile(\n",
    "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['sparse_categorical_accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzBYDJI0_Tfm"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "The `tf.contrib.tpu.keras_to_tpu_model` function converts a `tf.keras` model to an equivalent TPU version. We then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 828
    },
    "colab_type": "code",
    "id": "ExQ922tfzSGA",
    "outputId": "8e052895-4656-4c76-d063-a0729ab07a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.7.20.242:8470') for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5895018735270957403)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4427334198198249949)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 10731750899645578419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9618771642461069739)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 468142364918972154)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6800637295512838575)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 14349202036906006168)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4250147858927846172)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9500380147682033723)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10314060170985445080)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8910735954834311383)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 14208967849217245642)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Input text [31550891] Aromas include tropical fruit, broom, brimstone anEpoch 1/10\n",
      "\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 100), dtype=tf.int32, name='seed_10'), TensorSpec(shape=(128, 100, 1), dtype=tf.float32, name='time_distributed_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for seed\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 6.85379958152771 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 4.4170 - sparse_categorical_accuracy: 0.1432\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 3.1214 - sparse_categorical_accuracy: 0.1496\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 1.8877 - sparse_categorical_accuracy: 0.4430\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 1.1305 - sparse_categorical_accuracy: 0.6537\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 0.9944 - sparse_categorical_accuracy: 0.6912\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 0.9341 - sparse_categorical_accuracy: 0.7081\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 0.8993 - sparse_categorical_accuracy: 0.7179\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 0.8767 - sparse_categorical_accuracy: 0.7244\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 0.8613 - sparse_categorical_accuracy: 0.7289\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 0.8511 - sparse_categorical_accuracy: 0.7319\n",
      "INFO:tensorflow:Copying TPU weights to the CPU\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False)\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    training_model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
    "\n",
    "tpu_model.fit_generator(\n",
    "    training_generator(seq_len=100, batch_size=1024),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    ")\n",
    "tpu_model.save_weights('/tmp/bard.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCBtcpZkykSf"
   },
   "source": [
    "## Make predictions with the model\n",
    "\n",
    "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
    "Start the model off with a *seed* sentence, then generate 250 characters from it. We'll make five predictions from the initial seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "tU7M-EGGxR3E",
    "outputId": "50894b4f-e4f1-4ee4-ca2b-a6e1cd013e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      " is quite tannic, has enough personality, is short, grabby notes of cassis scents are followed by bright acidity and a bite of toast, and wood aromas aromas. Starts out sheer now, the wine is steely with intense acids cuts through the tone and oak has given the fruit flavors that streak to the finis\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      " features arapal high fruit and balancing cherry and ckdery tannins. It tastes of thicknimento that contributes to the medium body on the palate, with gorgeous tones lie ending in a spicy richness and freshness. It is ready to drink. This wine starts off with redcherry texture and lively acidity car\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      " of a coored golden surround balanncorice, yet textured and succulent wine, this broad wine brings a hint of more, brimming with big aromas of vanilla and dried fruit. Lighter than a halmockenh vintage, this sparkling wine has sharpherbill fruit, acidity ten or and lemon in the background. The palat\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      " is densely tannic, with aromas of lime, apple notes. Offers cherry, black berry, rose and tar are light on the palate of this wine, with plenty of fruit and mineral tones of tar and leather. From the producer's grilled strgetness from Chardonnay, this opens with red ly grassy, pepper and anise are \n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      " is developing, and should be shaped. Drink now, as much as well as ripe, this fermenting tannins support the wine from the slightest. Bready aromas of red cherry, herbs and licorice alongside firm tannins and alcohor to leave and nuances on the nose of this never measure. The tannins keep it sweet \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 300\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('/tmp/bard.h5')\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'the bottle of wine '\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "  prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "  last_word = predictions[-1]\n",
    "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "  next_idx = [\n",
    "      np.random.choice(256, p=next_probits[i])\n",
    "      for i in range(BATCH_SIZE)\n",
    "  ]\n",
    "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "  \n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  print('PREDICTION %d\\n\\n' % i)\n",
    "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "  generated = ''.join([chr(c) for c in p])\n",
    "  print(generated)\n",
    "  print()\n",
    "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "P_R6QsCU6KcD",
    "outputId": "3362662c-9918-4474-e92b-97932eda23c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87, 105, 110, 101,  32],\n",
       "       [ 87, 105, 110, 101,  32],\n",
       "       [ 87, 105, 110, 101,  32],\n",
       "       [ 87, 105, 110, 101,  32],\n",
       "       [ 87, 105, 110, 101,  32]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def transform(txt, pad_to=None):\n",
    "  # drop any non-ascii characters\n",
    "  output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "  if pad_to is not None:\n",
    "    output = output[:pad_to]\n",
    "    output = np.concatenate([\n",
    "        np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "        output,\n",
    "    ])\n",
    "  return output\n",
    "\n",
    "seed_txt = 'Wine '\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), 5, axis=0)\n",
    "seed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Review Generator with Cloud TPUs and Keras",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
